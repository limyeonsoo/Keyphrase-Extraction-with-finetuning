canonicalization of database records using adaptive similarity measures it is becoming increasingly common to construct databases from information automatically culled from many heterogeneous sources . for example , a research publication database can be constructed by automatically extracting titles , authors , and conference information from online papers . a common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways ( e.g. abbreviations , aliases , and misspellings ) . therefore , it can be difficult to construct a single , standard representation to present to the user . we refer to the task of constructing this representation as canonicalization . despite its importance , there is little existing work on canonicalization . in this paper , we explore the use of edit distance measures to construct a canonical representation that is `` central '' in the sense that it is most similar to each of the disparate records . this approach reduces the impact of noisy records on the canonical representation . furthermore , because the user may prefer different styles of canonicalization , we show how different edit distance costs can result in different forms of canonicalization . for example , reducing the cost of character deletions can result in representations that favor abbreviated forms over expanded forms ( e.g. kdd versus conference on knowledge discovery and data mining ) . we describe how to learn these costs from a small amount of manually annotated data using stochastic hill-climbing . additionally , we investigate feature-based methods to learn ranking preferences over canonicalizations . these approaches can incorporate arbitrary textual evidence to select a canonical record . we evaluate our approach on a real-world publications database and show that our learning method results in a canonicalization solution that is robust to errors and easily customizable to user preferences .