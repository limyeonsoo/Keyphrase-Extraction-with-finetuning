# Keyphrase-Extraction-with-finetuning
Using BERT Finetuning, Keyphrase Extraction in KDD,WWW and Inspec

Paper : https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE09410951&language=ko_KR

<div>
  <img height="400" src="https://user-images.githubusercontent.com/42796949/81039197-4be15380-8ee3-11ea-9349-70b031882659.png">
  <br><br>
  <img height="400" src="https://user-images.githubusercontent.com/42796949/81039200-4c79ea00-8ee3-11ea-8d88-8e28750b8f74.png">
</div>
  
Despite extensive research, performance enhancement of keyphrase (KP) extraction remains a challenging problem in modern informatics.  Recently, deep learning-based supervised approaches have exhibited  state-of-the-art accuracies with respect to this problem, and several of the proposed methods  utilize Bidirectional Encoder Representations from Transformers (BERT)-based language models. However, few studies have investigated the effective application of BERT-based fine-tuning techniques to the problem of KP extraction. In this study, we consider the aforementioned problem in the context of scientific articles by investigating the fine-tuning characteristics of two distinct BERT models â€” BERT (base) and SciBERT -based BERT. Three different datasets (WWW, KDD, and Inspec) comprising data obtained from the computer science domain are used to compare the results obtained by fine-tuning BERT and SciBERT in terms of KP extraction. Further, we visualize the internal structures of the two BERT models, and analyze them to explain the superiority of the performance of SciBERT in the KP extraction task .
